{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates how to use partiiton strategy to solve larger scale route optimization problem. The whole pipeline is illustrated by the below figure.\n",
    "\n",
    "<center><img src=../docs/media/pipeline.png width=\"100%\" /></center>\n",
    "\n",
    "There are 4 main steps in the pipeline:\n",
    "1.  Reduce\n",
    "2.  Partition\n",
    "3.  Solve\n",
    "4.  Merge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Setup some environment\n",
    "## 1.1.1 Load variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "ws_name = os.environ['AML_WORKSPACE_NAME']\n",
    "subscription_id = os.environ['AML_SUBSCRIPTION_ID']\n",
    "resource_group = os.environ['AML_RESOURCE_GROUP']\n",
    "tenant_id = os.environ['AML_TENANT_ID']\n",
    "min_nodes = int(os.environ['AML_MIN_NODES'])\n",
    "max_nodes = int(os.environ['AML_MAX_NODES'])\n",
    "aml_compute_target = os.environ['AML_COMPUTE_NAME']\n",
    "\n",
    "order_file = os.environ['MODEL_INPUT_ORDER_FILE']\n",
    "distance_file = os.environ['MODEL_INPUT_DISTANCE_FILE']\n",
    "model_output_path = os.environ['MODEL_OUTPUT_PATH']\n",
    "\n",
    "print('---- Check Azure setting ----')\n",
    "print(f'AML Workspace name       : {ws_name}')\n",
    "print(f'Subscription ID          : {subscription_id}')\n",
    "print(f'Resource group           : {resource_group}')\n",
    "print(f'tenant id                : {tenant_id}')\n",
    "print(f'min nodes of AML compute : {min_nodes}')\n",
    "print(f'max nodes of AML compute : {max_nodes}')\n",
    "print(f'AML compute target       : {aml_compute_target}')\n",
    "print(f'Model input order file   : {order_file}')\n",
    "print(f'Model input distance file: {distance_file}')\n",
    "print(f'Input output             : {model_output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.2 Azure authentication and Load Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az login\n",
    "# !az login --use-device-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_auth = AzureCliAuthentication()\n",
    "ws =  Workspace.get(name=ws_name\n",
    "                    ,subscription_id=subscription_id\n",
    "                    ,resource_group=resource_group\n",
    "                    ,auth=cli_auth)\n",
    "\n",
    "print(ws.get_details())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.3 Get Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve or create an Aml compute\n",
    "aml_compute_target = os.environ['AML_COMPUTE_NAME']\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = min_nodes, \n",
    "                                                                max_nodes = max_nodes)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.4 Create Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default datastore (Azure blob storage)\n",
    "def_blob_store = ws.get_default_datastore()\n",
    "\n",
    "# source directory\n",
    "source_directory = '../src/'\n",
    "print(f'Source code is in {source_directory} directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "env_name = 'op-env'\n",
    "\n",
    "try:\n",
    "    env = Environment.get(ws, env_name, 2)\n",
    "    print(\"Found existing environment.\")\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating new enviroment\")\n",
    "    env = Environment(env_name)\n",
    "\n",
    "    # enable Docker \n",
    "    env.docker.enabled = True\n",
    "    # set Docker base image to the default CPU-based image\n",
    "    env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "    # use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "    env.python.user_managed_dependencies = False\n",
    "    # specify CondaDependencies obj\n",
    "    env.python.conda_dependencies = CondaDependencies.create(\n",
    "            conda_packages=['pandas']\n",
    "            ,pip_packages=['ortools'\n",
    "                            ,'azureml-defaults']\n",
    "        )\n",
    "\n",
    "    env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# set environment\n",
    "run_config.environment = env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Set up Azure ML Pipeline\n",
    "## 1.2.1 Reduce the search space of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = Dataset.File.from_files((def_blob_store, order_file))\n",
    "distance = Dataset.File.from_files((def_blob_store, distance_file))\n",
    "\n",
    "# Naming the intermediate data \n",
    "model_result_partial = PipelineData(\"model_result_partial\",datastore=def_blob_store)\n",
    "model_input_reduced = PipelineData(\"model_input_reduced\",datastore=def_blob_store)\n",
    "\n",
    "reduce_step = PythonScriptStep(\n",
    "    script_name=\"reduce.py\", \n",
    "    arguments=[\"--model_input\", model_input.as_named_input('model_input').as_download(path_on_compute='order_file'),\n",
    "                \"--distance\", distance.as_named_input('distance').as_download(path_on_compute='distance_file'),\n",
    "                \"--model_result_partial\", model_result_partial,\n",
    "                \"--model_input_reduced\", model_input_reduced],\n",
    "    outputs=[model_result_partial, model_input_reduced],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.2 Partition the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming the intermediate data \n",
    "model_input_list = PipelineData(\"model_input_list\",datastore=def_blob_store).as_dataset()\n",
    "\n",
    "parition_step = PythonScriptStep(\n",
    "    script_name=\"partition.py\", \n",
    "    arguments=[\"--model_input_reduced\", model_input_reduced,\n",
    "                \"--distance\", distance.as_named_input('distance').as_download(path_on_compute='distance_file'),\n",
    "                \"--model_input_list\", model_input_list],\n",
    "    inputs=[model_input_reduced],\n",
    "    outputs=[model_input_list],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Solve individual problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Naming the intermediate data \n",
    "model_result_list = PipelineData(\"model_result_list\", datastore=def_blob_store)\n",
    "\n",
    "# pass distance file as side input\n",
    "local_path = \"/tmp/{}\".format(str(uuid.uuid4()))\n",
    "distance_config = distance.as_named_input(\"distance\").as_mount(local_path)\n",
    "\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=source_directory,\n",
    "    entry_script='solve.py',\n",
    "    mini_batch_size=\"1\",\n",
    "    error_threshold=-1,\n",
    "    output_action=\"append_row\",\n",
    "    append_row_file_name=\"model_result_list.txt\",\n",
    "    environment=env,\n",
    "    compute_target=aml_compute,\n",
    "    process_count_per_node=1,\n",
    "    node_count=1)\n",
    "\n",
    "solve_step = ParallelRunStep(\n",
    "    name=\"solve\",\n",
    "    inputs=[model_input_list.as_named_input('model_input_list')],\n",
    "    output=model_result_list,\n",
    "    arguments=[\"--distance\", distance_config],\n",
    "    side_inputs=[distance_config],\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4 Merge the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming the intermediate data \n",
    "model_result_final = OutputFileDatasetConfig(destination=(def_blob_store, 'model_result_final'))\n",
    "\n",
    "merge_step = PythonScriptStep(\n",
    "    script_name=\"merge.py\", \n",
    "    arguments=[\"--model_input\", model_input.as_named_input('model_input').as_download(path_on_compute='order_file'), \n",
    "    \"--distance\", distance.as_named_input('distance').as_download(path_on_compute='distance_file'),\n",
    "    \"--model_result_partial\", model_result_partial, \n",
    "    \"--model_result_list\", model_result_list, \n",
    "    \"--model_result_final\", model_result_final],\n",
    "    inputs=[model_result_partial, model_result_list],\n",
    "    outputs=[model_result_final],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Create the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[reduce_step, parition_step, solve_step, merge_step])\n",
    "\n",
    "print(\"Pipeline is built\")\n",
    "\n",
    "pipeline_run = Experiment(ws, 'optimization_example').submit(pipeline)\n",
    "print(\"Pipeline is submitted for execution\")\n",
    "\n",
    "RunDetails(pipeline_run).show()\n",
    "\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('optimization-ip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1dc112f8eedd872ac4b4a310e5d0e2b86038880c45a5c41440f57bee5a6027e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
