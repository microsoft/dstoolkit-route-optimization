{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "from azureml.contrib.pipeline.steps import ParallelRunStep, ParallelRunConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Setup some environment\n",
    "## 1.1.1 Load variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv('../.env'))\n",
    "\n",
    "ws_name = os.environ['AML_WORKSPACE_NAME']\n",
    "subscription_id = os.environ['AML_SUBSCRIPTION_ID']\n",
    "resource_group = os.environ['AML_RESOURCE_GROUP']\n",
    "tenant_id = os.environ['AML_TENANT_ID']\n",
    "min_nodes = int(os.environ['AML_MIN_NODES'])\n",
    "max_nodes = int(os.environ['AML_MAX_NODES'])\n",
    "aml_compute_target = os.environ['AML_COMPUTE_NAME']\n",
    "\n",
    "model_input_path = os.environ['MODEL_INPUT_PATH']\n",
    "order_file = os.environ['MODEL_INPUT_ORDER_FILE']\n",
    "distance_file = os.environ['MODEL_INPUT_DISTANCE_FILE']\n",
    "model_output_path = os.environ['MODEL_OUTPUT_PATH']\n",
    "\n",
    "print('---- Check Azure setting ----')\n",
    "print(f'AML Workspace name       : {ws_name}')\n",
    "print(f'Subscription ID          : {subscription_id}')\n",
    "print(f'Resource group           : {resource_group}')\n",
    "print(f'tenant id                : {tenant_id}')\n",
    "print(f'min nodes of AML compute : {min_nodes}')\n",
    "print(f'max nodes of AML compute : {max_nodes}')\n",
    "print(f'AML compute target       : {aml_compute_target}')\n",
    "print(f'Input path for models    : {model_input_path}')\n",
    "print(f'Model input order file   : {order_file}')\n",
    "print(f'Model input distance file: {distance_file}')\n",
    "print(f'Input output             : {model_output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.2 Azure authentication and Load Azure ML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!az login\n",
    "!az login --use-device-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_auth = AzureCliAuthentication()\n",
    "ws =  Workspace.get(name=ws_name\n",
    "                    ,subscription_id=subscription_id\n",
    "                    ,resource_group=resource_group\n",
    "                    ,auth=cli_auth)\n",
    "\n",
    "print(ws.get_details())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.3 Get Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve or create an Aml compute\n",
    "aml_compute_target = os.environ['AML_COMPUTE_NAME']\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = min_nodes, \n",
    "                                                                max_nodes = max_nodes)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.4 Create Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default datastore (Azure blob storage)\n",
    "def_blob_store = ws.get_default_datastore()\n",
    "\n",
    "# source directory\n",
    "source_directory = '../src/'\n",
    "print(f'Source code is in {source_directory} directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# environment\n",
    "env = Environment('op-env')\n",
    "\n",
    "# enable Docker \n",
    "env.docker.enabled = True\n",
    "# set Docker base image to the default CPU-based image\n",
    "env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "# use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "env.python.user_managed_dependencies = False\n",
    "# specify CondaDependencies obj\n",
    "env.python.conda_dependencies = CondaDependencies.create(\n",
    "        conda_packages=['pandas']\n",
    "        ,pip_packages=['ortools'\n",
    "                        ,'azureml-sdk']\n",
    "    )\n",
    "\n",
    "# set environment\n",
    "run_config.environment = env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Set up Azure ML Pipeline\n",
    "## 1.2.1 Reduce the search space of the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Order file\n",
    "orderpath = DataPath(datastore=def_blob_store, \n",
    "                path_on_datastore='model_input/order_small.csv')\n",
    "order_input = (PipelineParameter(name=\"order_data\", default_value=orderpath),\n",
    "                           DataPathComputeBinding(mode='mount'))\n",
    "\n",
    "## Distance file\n",
    "datapath = DataPath(datastore=def_blob_store, \n",
    "                path_on_datastore='model_input/distance.csv')\n",
    "distance_input = (PipelineParameter(name=\"distance_data\", default_value=datapath),\n",
    "                           DataPathComputeBinding(mode='mount'))\n",
    "\n",
    "## Intermediate files\n",
    "model_result_partial = OutputFileDatasetConfig(destination=(def_blob_store, 'reduced_result/'))\n",
    "model_input_reduced = OutputFileDatasetConfig(destination=(def_blob_store, 'reduced_result/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_step = PythonScriptStep(\n",
    "    script_name=\"reduce.py\",\n",
    "    source_directory=source_directory,\n",
    "    arguments=[\"--model_input\", order_input\n",
    "            ,\"--distance\", distance_input\n",
    "            ,\"--model_result_partial\", model_result_partial\n",
    "            ,\"--model_input_reduced\", model_input_reduced\n",
    "            ],\n",
    "    inputs=[order_input, distance_input],\n",
    "    outputs=[model_result_partial\n",
    "            , model_input_reduced],\n",
    "    compute_target=aml_compute,\n",
    "    runconfig=run_config,\n",
    "    allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[reduce_step])\n",
    "print(\"Pipeline is built\")\n",
    "\n",
    "pipeline_run = Experiment(ws, 'optimization_example_reduce_kyiwasak').submit(pipeline)\n",
    "print(\"Pipeline is submitted for execution\")\n",
    "\n",
    "#RunDetails(pipeline_run).show()\n",
    "\n",
    "pipeline_run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.2 Partition the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming the intermediate data \n",
    "model_input_list = OutputFileDatasetConfig(destination=(def_blob_store, 'partition_result/'))\n",
    "\n",
    "parition_step = PythonScriptStep(\n",
    "    script_name=\"partition.py\", \n",
    "    arguments=[\"--model_input_reduced\", model_input_reduced\n",
    "            ,\"--distance\", distance_input\n",
    "            , \"--model_input_list\", model_input_list],\n",
    "    inputs=[model_input_reduced, distance_input],\n",
    "    outputs=[model_input_list],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[reduce_step, parition_step])\n",
    "print(\"Pipeline is built\")\n",
    "\n",
    "pipeline_run = Experiment(ws, 'optimization_example_partition_kyiwasak').submit(pipeline)\n",
    "print(\"Pipeline is submitted for execution\")\n",
    "\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 (Under Construction) Solve individual problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming the intermediate data \n",
    "#model_result_list = PipelineData(\"model_result_list\", datastore=def_blob_store).as_dataset()\n",
    "\n",
    "model_result_list = OutputFileDatasetConfig(destination=(def_blob_store, 'solve_result/'))\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=source_directory,\n",
    "    entry_script='solve.py',\n",
    "    mini_batch_size=\"1\",\n",
    "    error_threshold=1,\n",
    "    output_action=\"append_row\",\n",
    "    environment=env,\n",
    "    compute_target=aml_compute,\n",
    "    node_count=max_nodes)\n",
    "\n",
    "solve_step = ParallelRunStep(\n",
    "    name=\"solve\",\n",
    "    inputs=[model_input_list, distance_input],\n",
    "    output=model_result_list,\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[reduce_step, parition_step, solve_step])\n",
    "print(\"Pipeline is built\")\n",
    "\n",
    "pipeline_run = Experiment(ws, 'optimization_example_solve_kyiwasak').submit(pipeline)\n",
    "print(\"Pipeline is submitted for execution\")\n",
    "\n",
    "RunDetails(pipeline_run).show()\n",
    "\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4 (Under Construction) Merge the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming the intermediate data \n",
    "model_result_final = PipelineData(\"model_result_final\",datastore=def_blob_store)\n",
    "\n",
    "merge_step = PythonScriptStep(\n",
    "    script_name=\"merge.py\", \n",
    "    arguments=[\"--model_input\", model_input, \"--model_result_partial\", model_result_partial, \"--model_result_list\", model_result_list, \"--model_result_final\", model_result_final],\n",
    "    inputs=[model_input, model_result_partial, model_result_list],\n",
    "    outputs=[model_result_final],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 (Under Construction) Create the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[reduce_step, parition_step, solve_step, merge_step])\n",
    "print(\"Pipeline is built\")\n",
    "\n",
    "pipeline_run = Experiment(ws, 'optimization_example_kyiwasak').submit(pipeline)\n",
    "print(\"Pipeline is submitted for execution\")\n",
    "\n",
    "RunDetails(pipeline_run).show()\n",
    "\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
