{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates how to use partiiton strategy to solve larger scale route optimization problem. The rationale for partitioning is that usually an optimization problem could be hard to solve given the NP-hard nature for most of the optimization problems. To trade-off the result optimality and running time, one can partition the big problem into many smaller problems, then solve each smaller problem, and finially combine all results as the final result. The whole pipeline is illustrated by the below figure.\n",
    "\n",
    "<img src=../docs/media/pipeline.png width=\"90%\" />\n",
    "\n",
    "There are 4 main steps in the pipeline:\n",
    "1.  Reduce: It will try to assign some of the orders to truck routes in a heuristic way. The remaining unscheduled order will be passed to the later steps for optimization. This step is optional, namely, one can bypass this step but let optimizer search solution for all orders. However, reducing the search space by  heuristic can significantly reduce the search space. This will make it easier for the oprimization solver to find a good solution.  \n",
    "2.  Partition: This is core step to partition the big problem into smaller problems. \n",
    "3.  Solve: This step is to solve individual small problem using whatever optimization solver.\n",
    "4.  Merge: This final step is to combine all results from each small problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Load libraries\n",
    "\n",
    "We use Azure ML pipeline for the implementation. Specifically, the partitioning step is done by the PrallelRunStep in Azure ML SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "from azureml.data import OutputFileDatasetConfig\n",
    "\n",
    "from azureml.pipeline.steps import ParallelRunStep, ParallelRunConfig\n",
    "from azureml.data.datapath import DataPath, DataPathComputeBinding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Setup some environment\n",
    "## 1.1.1 Load variables\n",
    "\n",
    "Some parameters are managed by environment variables.To specify your values, create a .env file in the root folder of the repository and set the values for the following parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "ws_name = os.environ['AML_WORKSPACE_NAME']\n",
    "subscription_id = os.environ['AML_SUBSCRIPTION_ID']\n",
    "resource_group = os.environ['AML_RESOURCE_GROUP']\n",
    "\n",
    "\n",
    "print('---- Check Azure setting ----')\n",
    "print(f'AML Workspace name       : {ws_name}')\n",
    "print(f'Subscription ID          : {subscription_id}')\n",
    "print(f'Resource group           : {resource_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.2 Azure authentication and Load Azure ML Workspace\n",
    "\n",
    "We use Azure CLI to authenticate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = ws_name)\n",
    "    print(ws.get_details())\n",
    "except:\n",
    "    print(\"Workspace not accessible. Change your parameters or create a new workspace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3 Get Compute Cluster\n",
    "\n",
    "Read the compute name from the environment varibale. If it doest not exist in the Azure ML workspace, a new compute target will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve or create an Aml compute\n",
    "aml_compute_target = 'opcluster'\n",
    "\n",
    "min_nodes = 0\n",
    "max_nodes = 10\n",
    "try:\n",
    "    aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"Creating new compute target\")\n",
    "    \n",
    "\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\",\n",
    "                                                                min_nodes = min_nodes, \n",
    "                                                                max_nodes = max_nodes)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.4 Create AML Environemnt and Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default datastore (Azure blob storage)\n",
    "def_blob_store = ws.get_default_datastore()\n",
    "\n",
    "# source directory\n",
    "source_directory = '../src/'\n",
    "print(f'Source code is in {source_directory} directory.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "env_name = 'op-env'\n",
    "\n",
    "try:\n",
    "    env = Environment.get(ws, env_name)\n",
    "    print(\"Found existing environment.\")\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating new enviroment\")\n",
    "    env = Environment(env_name)\n",
    "\n",
    "    # enable Docker \n",
    "    env.docker.enabled = True\n",
    "    # set Docker base image to the default CPU-based image\n",
    "    env.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "    # use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "    env.python.user_managed_dependencies = False\n",
    "    # specify CondaDependencies obj\n",
    "    env.python.conda_dependencies = CondaDependencies.create(\n",
    "            conda_packages=['pandas']\n",
    "            ,pip_packages=['ortools'\n",
    "                            ,'azureml-defaults']\n",
    "        )\n",
    "\n",
    "    env.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new runconfig object\n",
    "run_config = RunConfiguration()\n",
    "\n",
    "# set environment\n",
    "run_config.environment = env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Set up Azure ML Pipeline\n",
    "\n",
    "This section contains the main logic of the optimization pipeline.\n",
    "\n",
    "## 1.2.1 Reduce the search space of the problem\n",
    "\n",
    "The first step is to reduce the search space by assigning some of the orders based on heuristic. The detailed logic is implemented in the reduce.py. In general, if we use heuristic propoerly, we can achieve a good trade-off between result optimality and running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please replace it with the path you uploaded to Azure ML default datestore.\n",
    "order_file = 'model_input/order_small.csv' \n",
    "distance_file = 'model_input/distance.csv'\n",
    "\n",
    "model_input = Dataset.File.from_files((def_blob_store, order_file))\n",
    "distance = Dataset.File.from_files((def_blob_store, distance_file))\n",
    "\n",
    "# Naming the intermediate data \n",
    "model_result_partial = PipelineData(\"model_result_partial\",datastore=def_blob_store)\n",
    "model_input_reduced = PipelineData(\"model_input_reduced\",datastore=def_blob_store)\n",
    "\n",
    "reduce_step = PythonScriptStep(\n",
    "    script_name=\"reduce.py\", \n",
    "    arguments=[\"--model_input\", model_input.as_named_input('model_input').as_download(path_on_compute='order_file'),\n",
    "                \"--distance\", distance.as_named_input('distance').as_download(path_on_compute='distance_file'),\n",
    "                \"--model_result_partial\", model_result_partial,\n",
    "                \"--model_input_reduced\", model_input_reduced],\n",
    "    outputs=[model_result_partial, model_input_reduced],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.2 Partition the problem\n",
    "\n",
    "For large scale optimization problem, the problem space is just so big to solve practically. A commonly used idea is to partition the big problem into many smaller problems. Then solve each smaller problem individually and combine all the results as the final result. In some cases, the partition may not affect the result optimality, for example, in the route optimization problem, we can partition the orders by the delivery sources. In other cases, there will be trade-off between result optimality and running time when partitioning is applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naming the intermediate data \n",
    "model_input_list = PipelineData(\"model_input_list\",datastore=def_blob_store).as_dataset()\n",
    "\n",
    "parition_step = PythonScriptStep(\n",
    "    script_name=\"partition.py\", \n",
    "    arguments=[\"--model_input_reduced\", model_input_reduced,\n",
    "                \"--distance\", distance.as_named_input('distance').as_download(path_on_compute='distance_file'),\n",
    "                \"--model_input_list\", model_input_list],\n",
    "    inputs=[model_input_reduced],\n",
    "    outputs=[model_input_list],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.3 Solve individual problem\n",
    "\n",
    "After the problem is partitioned, we can solve each individul one by using whatever optimization solver. The optimization solver itself may leverage multi-process to speed up the search of result. This level of parallelism is totally controlled by the solver but not our Azure ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Naming the intermediate data \n",
    "model_result_list = PipelineData(\"model_result_list\", datastore=def_blob_store)\n",
    "\n",
    "# pass distance file as side input\n",
    "local_path = \"/tmp/{}\".format(str(uuid.uuid4()))\n",
    "distance_config = distance.as_named_input(\"distance\").as_mount(local_path)\n",
    "\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=source_directory,\n",
    "    entry_script='solve.py',\n",
    "    mini_batch_size=\"5\",\n",
    "    error_threshold=-1,\n",
    "    output_action=\"append_row\",\n",
    "    append_row_file_name=\"model_result_list.txt\",\n",
    "    environment=env,\n",
    "    compute_target=aml_compute,\n",
    "    run_invocation_timeout=600,\n",
    "    node_count=max_nodes)\n",
    "\n",
    "solve_step = ParallelRunStep(\n",
    "    name=\"solve\",\n",
    "    inputs=[model_input_list.as_named_input('model_input_list')],\n",
    "    output=model_result_list,\n",
    "    arguments=[\"--distance\", distance_config],\n",
    "    side_inputs=[distance_config],\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.4 Merge the results\n",
    "\n",
    "Once all the smaller problems are solved, we can combine the result as the final one. There could be chance to further optimize the result in this step in the case the previous partitioning will affect the global optimal. For example, one may combine two packages into the same truck from two seperated result if the combined one is more cost-efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please replace it with the path you uploaded to Azure ML default datestore.\n",
    "model_output_path = 'model_output'\n",
    "\n",
    "# Naming the intermediate data \n",
    "model_result_final = OutputFileDatasetConfig(destination=(def_blob_store, model_output_path))\n",
    "\n",
    "merge_step = PythonScriptStep(\n",
    "    script_name=\"merge.py\", \n",
    "    arguments=[\"--model_input\", model_input.as_named_input('model_input').as_download(path_on_compute='order_file'), \n",
    "    \"--distance\", distance.as_named_input('distance').as_download(path_on_compute='distance_file'),\n",
    "    \"--model_result_partial\", model_result_partial, \n",
    "    \"--model_result_list\", model_result_list, \n",
    "    \"--model_result_final\", model_result_final],\n",
    "    inputs=[model_result_partial, model_result_list],\n",
    "    outputs=[model_result_final],\n",
    "    compute_target=aml_compute, \n",
    "    source_directory=source_directory,\n",
    "    runconfig=run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.5 Run the Pipeline\n",
    "\n",
    "Finally, we chained all steps into a single Azure ML pipeline and submit it to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[reduce_step, parition_step, solve_step, merge_step])\n",
    "\n",
    "print(\"Pipeline is built\")\n",
    "\n",
    "pipeline_run = Experiment(ws, 'optimization_example').submit(pipeline)\n",
    "print(\"Pipeline is submitted for execution\")\n",
    "\n",
    "RunDetails(pipeline_run).show()\n",
    "\n",
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Check the Model Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = Dataset.Tabular.from_delimited_files(path=(def_blob_store, model_output_path))\n",
    "\n",
    "print(model_output.to_pandas_dataframe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Publish the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish the pipeline to Azure ML \n",
    "\n",
    "published_pipeline = pipeline_run.publish_pipeline(\n",
    "    name='Route Optimization', description='Demo for route optimization', version='1.0')\n",
    "\n",
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the endpoint of the pipeline\n",
    "\n",
    "rest_endpoint = published_pipeline.endpoint\n",
    "print(rest_endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test-lib')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa7d232911748e382575568d245b5f3ebefb2e3901fe7f97486091a7b9348bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
